{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.misc import imread\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import random as rand\n",
    "import numpy as np \n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from featuresourcer import FeatureSourcer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOG_SVM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.SVM = LinearSVC()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.sourcer = None\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.sourcer = FeatureSourcer(params)\n",
    "        print(\"processing parameters\", params)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _extract_features(self, imgs):\n",
    "        features = []\n",
    "\n",
    "        for img in imgs:\n",
    "            features.append(self.sourcer.features(img))\n",
    "\n",
    "        features = np.asarray(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def fit(self, imgs, y=None):\n",
    "        features = self._extract_features(imgs)\n",
    "\n",
    "        self.scaler.fit(features)\n",
    "        x = self.scaler.transform(features)\n",
    "\n",
    "        self.SVM.fit(x, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, imgs, y=None):\n",
    "        features = self._extract_features(imgs)\n",
    "        x = self.scaler.transform(features)\n",
    "\n",
    "        return self.SVM.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (96, 96)\n",
    "\n",
    "insect_imgs, non_insect_imgs = [], []\n",
    "insect_paths = glob.glob('datasets/cropped/insect/**/*.png')\n",
    "non_insect_paths = glob.glob('datasets/cropped/non_insect/*.png')\n",
    "\n",
    "for path in insect_paths: insect_imgs.append(cv2.resize(cv2.imread(path), (img_size, img_size)))\n",
    "for path in non_insect_paths: non_insect_imgs.append(cv2.resize(cv2.imread(path), (img_size, img_size)))\n",
    "\n",
    "insect_imgs, non_insect_imgs = np.asarray(insect_imgs), np.asarray(non_insect_imgs)\n",
    "total_insects, total_non_insects = insect_imgs.shape[0], non_insect_imgs.shape[0]\n",
    "\n",
    "x = np.vstack((insect_imgs, non_insect_imgs))\n",
    "y = np.hstack((np.ones(total_insects), np.zeros(total_non_insects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 1, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 2, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 6, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 8}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'yuv', 'number_of_orientations': 12, 'pixels_per_cell': 16}\n",
      "processing parameters {'bounding_box_size': 96, 'cells_per_block': 4, 'color_model': 'hsv', 'number_of_orientations': 6, 'pixels_per_cell': 8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating', estimator=HOG_SVM(),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'color_model': ['hsv', 'yuv'], 'bounding_box_size': [96], 'number_of_orientations': [6, 12], 'pixels_per_cell': [8, 16], 'cells_per_block': [1, 2, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_params = {\n",
    "  'color_model': ['hsv', 'yuv'],  # hsv, yuv\n",
    "  'bounding_box_size': [96],             # 64, 96, 128\n",
    "  'number_of_orientations': [6, 12],        # 6, 12\n",
    "  'pixels_per_cell': [8, 16],               # 8, 16\n",
    "  'cells_per_block': [1, 2, 4],                # 1, 2, 4\n",
    "}\n",
    "gs = GridSearchCV(HOG_SVM(), tuned_params, cv=3)\n",
    "\n",
    "gs.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 8.98433574,  2.31515598, 15.78239163,  5.80262804,  9.40904641,\n",
       "         3.30963564, 21.60641257,  7.84455522, 11.06377196,  3.81900493,\n",
       "        26.44073844,  4.66056212, 13.08859881,  2.73984806, 21.29920689,\n",
       "         5.0903813 , 13.5316093 ,  4.61028409, 28.153711  ,  7.85627341,\n",
       "        12.94740653,  3.28331431, 22.3935918 ,  6.46602567]),\n",
       " 'mean_score_time': array([ 5.34040562,  1.20608107,  9.53252602,  2.66566825,  4.44600526,\n",
       "         1.32905062, 10.56473867,  3.37838443,  4.8438894 ,  2.09428795,\n",
       "        13.47860853,  2.38384851,  5.75717862,  1.31680759,  9.90129805,\n",
       "         2.37143111,  6.23967902,  1.6528763 , 12.66163945,  3.04795702,\n",
       "         6.26885343,  1.61805081, 10.17097624,  2.67359694]),\n",
       " 'mean_test_score': array([0.63953488, 0.60465116, 0.70348837, 0.62790698, 0.63953488,\n",
       "        0.60465116, 0.70348837, 0.60465116, 0.73255814, 0.73837209,\n",
       "        0.73837209, 0.77325581, 0.69186047, 0.72674419, 0.75581395,\n",
       "        0.73837209, 0.79651163, 0.74418605, 0.79651163, 0.75581395,\n",
       "        0.73255814, 0.70930233, 0.77906977, 0.71511628]),\n",
       " 'mean_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'param_bounding_box_size': masked_array(data=[96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "                    96, 96, 96, 96, 96, 96, 96, 96, 96, 96],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_cells_per_block': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_color_model': masked_array(data=['hsv', 'hsv', 'hsv', 'hsv', 'yuv', 'yuv', 'yuv', 'yuv',\n",
       "                    'hsv', 'hsv', 'hsv', 'hsv', 'yuv', 'yuv', 'yuv', 'yuv',\n",
       "                    'hsv', 'hsv', 'hsv', 'hsv', 'yuv', 'yuv', 'yuv', 'yuv'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_number_of_orientations': masked_array(data=[6, 6, 12, 12, 6, 6, 12, 12, 6, 6, 12, 12, 6, 6, 12, 12,\n",
       "                    6, 6, 12, 12, 6, 6, 12, 12],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pixels_per_cell': masked_array(data=[8, 16, 8, 16, 8, 16, 8, 16, 8, 16, 8, 16, 8, 16, 8, 16,\n",
       "                    8, 16, 8, 16, 8, 16, 8, 16],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 1,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 2,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'hsv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 6,\n",
       "   'pixels_per_cell': 16},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 8},\n",
       "  {'bounding_box_size': 96,\n",
       "   'cells_per_block': 4,\n",
       "   'color_model': 'yuv',\n",
       "   'number_of_orientations': 12,\n",
       "   'pixels_per_cell': 16}],\n",
       " 'rank_test_score': array([19, 22, 16, 21, 19, 22, 16, 22, 11,  8,  8,  4, 18, 13,  5,  8,  1,\n",
       "         7,  1,  5, 11, 15,  3, 14], dtype=int32),\n",
       " 'split0_test_score': array([0.63793103, 0.55172414, 0.74137931, 0.65517241, 0.65517241,\n",
       "        0.60344828, 0.77586207, 0.67241379, 0.70689655, 0.63793103,\n",
       "        0.72413793, 0.70689655, 0.70689655, 0.70689655, 0.79310345,\n",
       "        0.79310345, 0.74137931, 0.63793103, 0.72413793, 0.68965517,\n",
       "        0.68965517, 0.72413793, 0.82758621, 0.75862069]),\n",
       " 'split0_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split1_test_score': array([0.68421053, 0.63157895, 0.78947368, 0.64912281, 0.68421053,\n",
       "        0.63157895, 0.78947368, 0.59649123, 0.78947368, 0.8245614 ,\n",
       "        0.84210526, 0.84210526, 0.78947368, 0.77192982, 0.84210526,\n",
       "        0.78947368, 0.80701754, 0.8245614 , 0.84210526, 0.8245614 ,\n",
       "        0.78947368, 0.73684211, 0.84210526, 0.70175439]),\n",
       " 'split1_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split2_test_score': array([0.59649123, 0.63157895, 0.57894737, 0.57894737, 0.57894737,\n",
       "        0.57894737, 0.54385965, 0.54385965, 0.70175439, 0.75438596,\n",
       "        0.64912281, 0.77192982, 0.57894737, 0.70175439, 0.63157895,\n",
       "        0.63157895, 0.84210526, 0.77192982, 0.8245614 , 0.75438596,\n",
       "        0.71929825, 0.66666667, 0.66666667, 0.68421053]),\n",
       " 'split2_train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'std_fit_time': array([0.61438561, 0.1157727 , 1.45227292, 0.32267663, 1.01709458,\n",
       "        0.72080359, 4.17531329, 2.39171358, 1.37403048, 1.37257364,\n",
       "        2.52937579, 0.36854384, 2.13317526, 0.03915764, 0.99489065,\n",
       "        0.92980344, 2.55574832, 1.34084116, 8.46610577, 1.03978949,\n",
       "        1.686077  , 0.36122121, 1.38706727, 0.41225249]),\n",
       " 'std_score_time': array([1.21326186, 0.13110284, 2.19416083, 0.02497148, 0.62562111,\n",
       "        0.12961161, 3.31164462, 0.54893269, 0.20980351, 0.89839498,\n",
       "        2.41765801, 0.3634067 , 1.17220757, 0.02095601, 0.80652425,\n",
       "        0.22760801, 1.65003992, 0.19947687, 1.94886067, 0.46516502,\n",
       "        0.42221916, 0.14899664, 1.79893478, 0.51239427]),\n",
       " 'std_test_score': array([0.03572532, 0.03775192, 0.08985765, 0.03455744, 0.04427637,\n",
       "        0.02144137, 0.11252041, 0.05286929, 0.04012511, 0.07712776,\n",
       "        0.0792088 , 0.05528469, 0.08636531, 0.03188124, 0.08972955,\n",
       "        0.07519976, 0.04183835, 0.07875974, 0.05211451, 0.05516231,\n",
       "        0.04186254, 0.03046263, 0.07935697, 0.03184209]),\n",
       " 'std_train_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965116279069767"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounding_box_size': 96,\n",
       " 'cells_per_block': 4,\n",
       " 'color_model': 'hsv',\n",
       " 'number_of_orientations': 6,\n",
       " 'pixels_per_cell': 8}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models...\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "print (\"Saving models...\")\n",
    "\n",
    "joblib.dump(gs.best_estimator_.SVM, 'models/svc2.pkl')\n",
    "joblib.dump(gs.best_estimator_.scaler, 'models/scaler2.pkl')\n",
    "\n",
    "print(\"...Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,6854400) (23328,) (1,6854400) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-017fc0f0b6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/full/other/Butterfly/original.jpeg?1532741436.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victoraubin/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,6854400) (23328,) (1,6854400) "
     ]
    }
   ],
   "source": [
    "\n",
    "from binaryclassifier import BinaryClassifier\n",
    "\n",
    "sourcer_params = {\n",
    "    'bounding_box_size': 96,\n",
    "    'cells_per_block': 4,\n",
    "    'color_model': 'hsv',\n",
    "    'number_of_orientations': 6,\n",
    "    'pixels_per_cell': 8\n",
    "}\n",
    "\n",
    "cls = BinaryClassifier(gs.best_estimator_.SVM, gs.best_estimator_.scaler)\n",
    "src = FeatureSourcer(sourcer_params)\n",
    "\n",
    "img = cv2.imread(\"datasets/full/other/Butterfly/original.jpeg?1532741436.jpg\")\n",
    "features = src.features(img)\n",
    "f = gs.best_estimator_.scaler.transform([features.resize()])\n",
    "gs.best_estimator_.SVM.predict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
