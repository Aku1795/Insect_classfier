{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid_search.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q3UBJ4vHX2ET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import IPython\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "np.random.seed(43) # to make the results reproductible\n",
        "tf.set_random_seed(42) # to make the results reproductible "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mo7_0_FwXz1s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HabVV_V3YT9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 Importing images and setting up train test data"
      ]
    },
    {
      "metadata": {
        "id": "bbKjrVr3u_nb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Importing images and creating training, validation and test datasets"
      ]
    },
    {
      "metadata": {
        "id": "WQPAiJ9VQ6Up",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flags:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.image_size = 28\n",
        "    self.train_split = [0.75*0.75,0.25*0.75,0.25]\n",
        "    self.fixed_img_number = 700\n",
        "\n",
        "flags = Flags()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qjy-hUMIuOhH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/gdrive/My Drive/DL project_2019/Raw_Dataset'\n",
        "\n",
        "# Get the filenames and label of our data\n",
        "image_filenames = []\n",
        "image_labels = []\n",
        "species = ['Bees','Mosquitoes', 'Flies', 'Butterflies']\n",
        "for label, category in enumerate(species):\n",
        "    image_names = os.listdir(os.path.join(data_dir, category))\n",
        "    image_names = sorted(image_names) # to make the results reproductibles\n",
        "    image_names = [x for x in image_names if os.stat(os.path.join(data_dir, category, x)).st_size != 0]\n",
        "    \n",
        "    image_names = image_names[:flags.fixed_img_number]\n",
        "    image_filenames += [os.path.join(\n",
        "        data_dir, category, image_name) for image_name in image_names]\n",
        "    \n",
        "    image_labels += [label] * len(image_names)\n",
        "    \n",
        "# Split data in three for training, validation and test\n",
        "train_image_filenames, train_image_labels = [], []\n",
        "valid_image_filenames, valid_image_labels = [], []\n",
        "test_image_filenames, test_image_labels  = [], []\n",
        "\n",
        "for image_filename, image_label in zip(image_filenames, image_labels):\n",
        "\n",
        "    x = np.random.choice(['train', 'valid', 'test'], p=flags.train_split)\n",
        "\n",
        "    if x == 'train':\n",
        "        train_image_filenames.append(image_filename)\n",
        "        train_image_labels.append(image_label)\n",
        "    if x == 'valid':\n",
        "        valid_image_filenames.append(image_filename)\n",
        "        valid_image_labels.append(image_label)\n",
        "    if x == 'test':\n",
        "        test_image_filenames.append(image_filename)\n",
        "        test_image_labels.append(image_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQHQOGPavLWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Defining Iterators for batch training"
      ]
    },
    {
      "metadata": {
        "id": "zjL8L7qOvIbp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Iterator builder \n",
        "def make_iterator(filenames, labels, batch_size, shuffle_and_repeat=False):\n",
        "    \"\"\"function that creates a `tf.data.Iterator` object\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    if shuffle_and_repeat:\n",
        "        dataset = dataset.apply(\n",
        "            tf.data.experimental.shuffle_and_repeat(buffer_size=1000))\n",
        "\n",
        "    def parse(filename, label):\n",
        "        \"\"\"function that reads the image and normalizes it\"\"\"\n",
        "        try:\n",
        "          image = tf.read_file(filename)\n",
        "          image = tf.image.decode_jpeg(image, channels = 3)\n",
        "          image = tf.cast(image, tf.float32)\n",
        "          image = tf.image.resize(image, (flags.image_size,flags.image_size))\n",
        "          image = image / 256\n",
        "          return {'image': image, 'label': label}\n",
        "        except Exception:\n",
        "          print(filename)\n",
        "\n",
        "    dataset = dataset.apply(tf.data.experimental.map_and_batch(\n",
        "        map_func=parse, batch_size=batch_size, num_parallel_batches=8))\n",
        "\n",
        "    if shuffle_and_repeat:\n",
        "        return dataset.make_one_shot_iterator()\n",
        "    else:\n",
        "        return dataset.make_initializable_iterator()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVGivZX8k00p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Constructing CNN "
      ]
    },
    {
      "metadata": {
        "id": "E2MpvvCUk4XW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def model_builder(cfg):\n",
        "  \"\"\"\n",
        "  cfg :list: list of hyperparameters on which to build the model\n",
        "  returns a Keras.Model object\n",
        "  \"\"\"\n",
        "  \n",
        "  kernel_size, strides, pool_size, dropout_1, dropout_2, optimizer, loss, batch_size, epochs = cfg\n",
        "  \n",
        "  #Model Architecture\n",
        "  model = keras.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(kernel_size, kernel_size)\n",
        "                   , activation='relu'\n",
        "                   , input_shape=(flags.image_size, flags.image_size, 3)))\n",
        "  model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides = strides, padding = 'valid'))\n",
        "  model.add(Conv2D(64, (kernel_size, kernel_size), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides = strides, padding = 'valid'))\n",
        "  model.add(Dropout(dropout_1))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(dropout_2))\n",
        "  model.add(Dense(len(species), activation='softmax'))\n",
        "  \n",
        "  #Model compilation\n",
        "  model.compile(optimizer=optimizer, \n",
        "              loss= loss,\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  #Instanciating train and val iterators \n",
        "  train_iterator = make_iterator(train_image_filenames, train_image_labels,\n",
        "    batch_size=batch_size, shuffle_and_repeat=True)\n",
        "  val_iterator = make_iterator(valid_image_filenames, valid_image_labels,\n",
        "    batch_size=batch_size, shuffle_and_repeat=True)\n",
        "  \n",
        "  #Model training\n",
        "  features = train_iterator.get_next()\n",
        "  images, labels = itemgetter('image', 'label')(features)\n",
        "\n",
        "  val_features = val_iterator.get_next()\n",
        "  val_images, val_labels = itemgetter('image', 'label')(val_features)\n",
        "  \n",
        "  history = model.fit(images\n",
        "                    , labels\n",
        "                    , epochs=epochs\n",
        "                    , validation_data = (val_images, val_labels)\n",
        "                    , steps_per_epoch= len(train_image_labels) // batch_size\n",
        "                    , validation_steps = len(valid_image_labels) // batch_size\n",
        "                   ) \n",
        "  \n",
        "  return history, model\n",
        "\n",
        "\n",
        "def config_builder():\n",
        "  \"\"\"\n",
        "  returns a list of lists containing configuration\n",
        "  \"\"\"\n",
        "  \n",
        "  configs = list()\n",
        "  kernel_size = [3,5]\n",
        "  strides = [2,4]\n",
        "  pool_size = [2,4]\n",
        "  dropout_1= [0.25,0.5]\n",
        "  dropout_2 = [0.25, 0.5]\n",
        "  optimizer = ['Adam', 'Adamax', 'rmsprop', 'Nadam']\n",
        "  loss = ['sparse_categorical_crossentropy']\n",
        "  batch_size = [10]\n",
        "  epochs = [15,20]\n",
        "  for a in kernel_size:\n",
        "    for b in strides:\n",
        "      for c in pool_size:\n",
        "        for d in dropout_1:\n",
        "          for e in dropout_2:\n",
        "            for f in optimizer:\n",
        "              for g in loss:\n",
        "                for h in batch_size:\n",
        "                  for i in epochs:\n",
        "                    cfg = [a,b,c,d,e,f,g,h,i]\n",
        "                    configs.append(cfg)\n",
        "  return configs\n",
        "\n",
        "\n",
        "def grid_search(configs):\n",
        "  \"\"\"\n",
        "  configs:list: list of lists containing hyperparametes\n",
        "  returns logs and models\n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  results = pd.DataFrame(columns =['config', 'train_acc', 'train_loss', 'test_acc', 'test_loss'])\n",
        "  histories = {}\n",
        "  models = {}\n",
        "  for cfg in configs:\n",
        "    str_cfg = '_'.join([str(x) for x in cfg])\n",
        "    print('config ', str_cfg)\n",
        "    history, model = model_builder(cfg)\n",
        "    row = {'config': [str_cfg]\n",
        "           ,'train_acc' : [history.history['acc'][-1]]\n",
        "          , 'train_loss' : [history.history['loss'][-1]]\n",
        "          , 'test_acc' : [history.history['val_acc'][-1]]\n",
        "          , 'test_loss' : [history.history['val_loss'][-1]]\n",
        "          }\n",
        "\n",
        "    models[str_cfg] = model\n",
        "    row = pd.DataFrame.from_dict(row, orient = 'columns')\n",
        "    results = pd.concat([results, row], ignore_index = True)\n",
        "    \n",
        "    #saving logs\n",
        "    saving_dir = '/gdrive/My Drive/DL project_2019/logs_CNN_from_scratch/Grid_search'\n",
        "    histories[str_cfg] = history.history\n",
        "    for key in histories[str_cfg]:\n",
        "      content = histories[str_cfg][key]\n",
        "      histories[str_cfg][key] = [str(x) for x in content]\n",
        "    hist = json.dumps(histories)\n",
        "    name = \"20190404_2_histories.json\"\n",
        "    saving_name = os.path.join(saving_dir, name )\n",
        "    f = open(saving_name,\"w\")\n",
        "    f.write(hist)\n",
        "    f.close()\n",
        "    \n",
        "    #saving summary of each session\n",
        "    name = \"20190404_2_summary.csv\"\n",
        "    saving_name = os.path.join(saving_dir, name )\n",
        "    results.to_csv(saving_name)\n",
        "    \n",
        "  return results, models, histories\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRK4CQKwllSP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Training models"
      ]
    },
    {
      "metadata": {
        "id": "a7mJfoRmzJFG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "configs = config_builder()\n",
        "results, models, histories = grid_search(configs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}